{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORFN0G3iENRFQXTDmkHNi3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/databyhuseyn/NLP/blob/main/GPT_2_alt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1"
      ],
      "metadata": {
        "id": "mUicfJVXZ6pm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5kBL21rN5VlK"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import inspect ### NEW"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Uhvv0dQWZh8i"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.random.rand(4, 5, 3, 2)\n",
        "A = torch.tensor(A)\n",
        "print(A.shape)\n",
        "A.transpose(-2, -1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHoKY_fJZjtk",
        "outputId": "6b38a851-57f9-46a0-8bed-83e56d974b71"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 5, 3, 2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 5, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    # key, query, value projections for all heads but in a batch\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "    # output projection\n",
        "    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "    # regularization\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
        "    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size,\n",
        "                                                       config.block_size)).view(1, 1, config.block_size,\n",
        "                                                                                config.block_size))\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "    # calculate query, key, values for all head in batch and move head forward to be the batch\n",
        "    # nh is 'number of heads', hs is 'head size' and C (number of channels) = nh * hs\n",
        "    # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh * hs=C=768 channels in the Transformer\n",
        "    qkv = self.c_attn(x)\n",
        "    q, k , v = qkv.split(self.n_embd, dim=2)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "    # attention (materializes the large (T,T) matrix for al the queries and keys)\n",
        "\n",
        "    ### NEW\n",
        "    # att = (q @ k.transpose(-2, -1)) * (1.0 / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32)))\n",
        "    # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "    # att = F.softmax(att, dim=-1)\n",
        "    # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) --> (B, nh, T, hs)\n",
        "    y = F.scaled_dot_product_attention(q, k, v, is_causal=True)   ### NEW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "    # output projection\n",
        "    y = self.c_proj(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "N9e42daWXlYv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "    self.gelu    = nn.GELU(approximate='tanh')\n",
        "    self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.c_fc(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.c_proj(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ts6C0OkW-W1A"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "Wq_a591X9Ajs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size: int = 1024 # max sequence length\n",
        "  vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|>\n",
        "  n_layer: int = 12\n",
        "  n_head: int = 12\n",
        "  n_embd: int = 768"
      ],
      "metadata": {
        "id": "k97qsq8_7wBQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.transformer = nn.ModuleDict(dict(\n",
        "        wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "        wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "        ln_f = nn.LayerNorm(config.n_embd)\n",
        "    ))\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "    # weight sharing scheme\n",
        "    self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "    # init params\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self,module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      std=0.02\n",
        "      if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "        std *= (2 * self.config.n_layer) ** -0.5\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0,std=std)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0,std=0.02)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx is of shape (B, T)\n",
        "    B, T = idx.size()\n",
        "    assert T <= self.config.block_size, f\"Can not forward sequence of length {T}, block size is exceeded\"\n",
        "    # forward the token and position embeddings\n",
        "    pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "    pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "    tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "    x = tok_emb + pos_emb\n",
        "    # forward the blocks of the transformer\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x)\n",
        "    # forward the final layernorm and the classifier\n",
        "    x = self.transformer.ln_f(x)\n",
        "    logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "    return logits, loss\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, model_type):\n",
        "    \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "    assert model_type in {'gpt2','gpt2-medium','gpt2-large','gpt2-xl'}\n",
        "    from transformers import GPT2LMHeadModel\n",
        "    print(\"Loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "    # n_layer, n_head, and n_embd are determined from model_type\n",
        "    config_args = {\n",
        "          'gpt2':         dict(n_layer = 12, n_head = 12, n_embd =768),   # 124M params\n",
        "          'gpt2-medium':  dict(n_layer = 24, n_head = 16, n_embd =1024),  # 350M params\n",
        "          'gpt2-large':   dict(n_layer = 36, n_head = 20, n_embd =1280),  # 774M params\n",
        "          'gpt2-xl':      dict(n_layer = 48, n_head = 25, n_embd =1600),  # 1558M params\n",
        "      }[model_type]\n",
        "    config_args['vocab_size']=50257 # always 50257 for GPT model checkpoints\n",
        "    config_args['block_size']=1024 # always 1024 for GPT model checkpoints\n",
        "    # create a from-scratch initializerd minGPT model\n",
        "    config = GPTConfig(**config_args)\n",
        "    model = GPT(config)\n",
        "    sd = model.state_dict()\n",
        "    sd_keys = sd.keys()\n",
        "    sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]  # discard this mask / buffer_reg\n",
        "\n",
        "    # init a huggingface/transformers model\n",
        "    model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "    sd_hf = model_hf.state_dict()\n",
        "\n",
        "    # copy while ensuring all of the parameters a re aligned and match in names and shapes\n",
        "    sd_keys_hf = sd_hf.keys()\n",
        "    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
        "    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
        "    transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "    # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla\n",
        "    # this means that we have to transpose these weights when we import them\n",
        "    assert len(sd_keys_hf) == len(sd_keys), f'mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}'\n",
        "    for k in sd_keys_hf:\n",
        "      if any(k.endswith(w) for w in transposed):\n",
        "        assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "          sd[k].copy_(sd_hf[k].t())\n",
        "      else:\n",
        "        assert sd_hf[k].shape == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "          sd[k].copy_(sd_hf[k])\n",
        "\n",
        "    return model\n",
        "\n",
        "  ### NEW\n",
        "  def configure_optimizers(self, weight_decay, learning_rate, device):\n",
        "    # start with all of the candidate parameters (that require grad)\n",
        "    param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "    # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "    # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't\n",
        "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "    optim_groups = [\n",
        "        {'params': decay_params, 'weight_decay': weight_decay},\n",
        "        {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "    ]\n",
        "    num_decay_params = sum(p.numel() for p in decay_params)\n",
        "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "    print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} params\")\n",
        "    print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} params\")\n",
        "    # Create AdamW optimizer and use the fused version if it is available\n",
        "    fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "    use_fused = fused_available and 'cuda' in device\n",
        "    print(f'using fused AdamW: {use_fused}')\n",
        "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "eA7f3gVc8All"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture results\n",
        "model = GPT.from_pretrained('gpt2')\n",
        "print(\"YAAAYY DIDN'T CRASH\")"
      ],
      "metadata": {
        "id": "WOZz4unAMTly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff0c02f-5aa7-4e9f-9bdf-91b809c4c44f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights from pretrained gpt: gpt2\n",
            "YAAAYY DIDN'T CRASH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyZTZuUDhe0V",
        "outputId": "f27f7bc1-6db6-442c-e250-a07c3ccb8663"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-20 07:27:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-09-20 07:27:05 (32.4 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "data = text[:1000]\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOdeTXXxhw7B",
        "outputId": "c6390080-9053-4f23-f6f0-9a7d35069a5d",
        "collapsed": true
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tiktoken\n",
        "\n",
        "class DataLoaderLite:\n",
        "  def __init__(self, B, T):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "\n",
        "    # at init load tokens from disk and store them in memory\n",
        "    with open('input.txt', 'r') as f:\n",
        "      text = f.read()\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensor(tokens)\n",
        "    print(f'loaded {len(self.tokens)} tokens')\n",
        "    print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "\n",
        "    self.current_position = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "    buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
        "    x = (buf[:-1]).view(B, T)\n",
        "    y = (buf[1:]).view(B, T)\n",
        "    # advance the position in the tensor\n",
        "    self.current_position += B * T\n",
        "    # if loading the next batch would be out of bounds, reset\n",
        "    if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "      self.current_position = 0\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "aUdlo0iwohV5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "elif hasattr(torch.backends,'mps') and torch.backends.mps.torch.cuda.is_available():\n",
        "  device = 'mps'\n",
        "print(f'using device: {device}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JCVdFMFh-DN",
        "outputId": "9ab156ca-a3e6-4864-a98e-e2fa176071a9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device = 'cpu'\n",
        "# import tiktoken\n",
        "# enc = tiktoken.get_encoding('gpt2')\n",
        "# with open('input.txt', 'r') as f:\n",
        "#   text = f.read()\n",
        "# data = text[:1000]\n",
        "# tokens = enc.encode(text)\n",
        "# B, T = 4, 32\n",
        "# buf = torch.tensor(tokens[:B*T + 1])\n",
        "# buf = buf.to(device)\n",
        "# x = buf[:-1].view(B, T)\n",
        "# y = buf[1:].view(B, T)"
      ],
      "metadata": {
        "id": "UgP3UISBkbvG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(1337)"
      ],
      "metadata": {
        "id": "eS5zxW4ovVzt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# B16 / T 1024\n",
        "train_loader = DataLoaderLite(B = 4, T = 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU2Yep1eq3XC",
        "outputId": "3fc3504e-ce79-47bc-eb59-73581c24c29e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 338025 tokens\n",
            "1 epoch = 2640 batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision('high')"
      ],
      "metadata": {
        "id": "-x9EW-01GH1Q"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "8VtGSk68DMbw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "# logits, loss = model(x, y)\n",
        "# print(loss)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)   ### NEW BETAS and EPS\n",
        "for i in range(50):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()    # later\n",
        "  x, y = x.to(device), y.to(device)   # later\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = model(x, y)\n",
        "  # import code; code.interact(local=locals())\n",
        "  loss.backward()\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1)    ### NEW\n",
        "  optimizer.step()\n",
        "  # torch.cuda.synchronize()     # gpu stops\n",
        "  t1 = time.time()\n",
        "  dt = (t1 - t0) * 1000    # time difference in milliseconds\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "  print(f'step {i}, loss: {loss.item()}, dt: {dt:.2f}ms, tok/sec: {tokens_per_sec:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbmZo0MvlDcr",
        "outputId": "9e463770-04a7-4750-ab19-71e45b346b51"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, loss: 10.960028648376465, dt: 1397.85ms, tok/sec: 91.57\n",
            "step 1, loss: 9.687808990478516, dt: 62.87ms, tok/sec: 2035.83\n",
            "step 2, loss: 9.266006469726562, dt: 90.98ms, tok/sec: 1406.88\n",
            "step 3, loss: 9.146829605102539, dt: 89.65ms, tok/sec: 1427.79\n",
            "step 4, loss: 8.658792495727539, dt: 65.93ms, tok/sec: 1941.40\n",
            "step 5, loss: 8.359376907348633, dt: 35.41ms, tok/sec: 3615.15\n",
            "step 6, loss: 8.897807121276855, dt: 33.28ms, tok/sec: 3846.22\n",
            "step 7, loss: 8.86855411529541, dt: 44.18ms, tok/sec: 2897.04\n",
            "step 8, loss: 8.11693000793457, dt: 32.47ms, tok/sec: 3941.99\n",
            "step 9, loss: 7.999248504638672, dt: 33.13ms, tok/sec: 3863.85\n",
            "step 10, loss: 8.325180053710938, dt: 45.21ms, tok/sec: 2831.52\n",
            "step 11, loss: 7.360818862915039, dt: 35.97ms, tok/sec: 3558.43\n",
            "step 12, loss: 7.7298712730407715, dt: 33.70ms, tok/sec: 3798.49\n",
            "step 13, loss: 7.399248123168945, dt: 72.16ms, tok/sec: 1773.92\n",
            "step 14, loss: 7.489315032958984, dt: 35.98ms, tok/sec: 3557.72\n",
            "step 15, loss: 7.342123508453369, dt: 37.60ms, tok/sec: 3404.04\n",
            "step 16, loss: 7.395068168640137, dt: 34.11ms, tok/sec: 3752.35\n",
            "step 17, loss: 8.247725486755371, dt: 32.68ms, tok/sec: 3916.54\n",
            "step 18, loss: 7.106740951538086, dt: 34.01ms, tok/sec: 3763.34\n",
            "step 19, loss: 7.818636417388916, dt: 31.82ms, tok/sec: 4022.98\n",
            "step 20, loss: 7.422550201416016, dt: 33.81ms, tok/sec: 3786.17\n",
            "step 21, loss: 7.706119537353516, dt: 35.02ms, tok/sec: 3655.12\n",
            "step 22, loss: 6.343473434448242, dt: 32.40ms, tok/sec: 3950.83\n",
            "step 23, loss: 12.506852149963379, dt: 34.56ms, tok/sec: 3703.96\n",
            "step 24, loss: 6.926457405090332, dt: 39.18ms, tok/sec: 3266.73\n",
            "step 25, loss: 6.629554271697998, dt: 41.12ms, tok/sec: 3112.75\n",
            "step 26, loss: 6.7340826988220215, dt: 33.84ms, tok/sec: 3782.35\n",
            "step 27, loss: 7.584292411804199, dt: 34.16ms, tok/sec: 3746.87\n",
            "step 28, loss: 7.090334892272949, dt: 38.05ms, tok/sec: 3363.98\n",
            "step 29, loss: 6.911832809448242, dt: 34.17ms, tok/sec: 3746.27\n",
            "step 30, loss: 6.925878524780273, dt: 33.15ms, tok/sec: 3861.19\n",
            "step 31, loss: 7.180572509765625, dt: 43.51ms, tok/sec: 2942.16\n",
            "step 32, loss: 7.0798234939575195, dt: 55.29ms, tok/sec: 2314.93\n",
            "step 33, loss: 6.90499210357666, dt: 33.55ms, tok/sec: 3815.44\n",
            "step 34, loss: 7.930250644683838, dt: 34.42ms, tok/sec: 3718.97\n",
            "step 35, loss: 7.769459247589111, dt: 33.46ms, tok/sec: 3825.61\n",
            "step 36, loss: 7.58781099319458, dt: 56.57ms, tok/sec: 2262.60\n",
            "step 37, loss: 7.553841590881348, dt: 40.66ms, tok/sec: 3147.90\n",
            "step 38, loss: 7.8387298583984375, dt: 31.99ms, tok/sec: 4001.15\n",
            "step 39, loss: 7.489419937133789, dt: 34.26ms, tok/sec: 3735.69\n",
            "step 40, loss: 7.406498908996582, dt: 33.93ms, tok/sec: 3772.39\n",
            "step 41, loss: 6.718770980834961, dt: 33.88ms, tok/sec: 3778.25\n",
            "step 42, loss: 6.943262577056885, dt: 55.44ms, tok/sec: 2308.62\n",
            "step 43, loss: 6.946837902069092, dt: 41.30ms, tok/sec: 3099.38\n",
            "step 44, loss: 6.872673988342285, dt: 47.24ms, tok/sec: 2709.61\n",
            "step 45, loss: 6.883339881896973, dt: 64.80ms, tok/sec: 1975.45\n",
            "step 46, loss: 6.105765342712402, dt: 68.18ms, tok/sec: 1877.38\n",
            "step 47, loss: 6.151416301727295, dt: 91.67ms, tok/sec: 1396.35\n",
            "step 48, loss: 6.883740425109863, dt: 63.98ms, tok/sec: 2000.77\n",
            "step 49, loss: 6.710459232330322, dt: 42.81ms, tok/sec: 2989.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NEW\n",
        "max_lr = 3e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "def get_lr(it):\n",
        "  # 1) linear warmup for warmup_iters steps\n",
        "  if it<warmup_steps:\n",
        "    return max_lr * (it+1) / warmup_steps\n",
        "  # 2) if it > lr_decay_ites, return min learning rate\n",
        "  if it > max_steps:\n",
        "    return min_lr\n",
        "  # 3) in between, use cosine decay down to min learning rate\n",
        "  decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "  assert 0 <= decay_ratio <= 1\n",
        "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))   # coeff starts at 1 and goes to 0\n",
        "  return min_lr + coeff * (max_lr - min_lr)"
      ],
      "metadata": {
        "id": "_RyrUaOZIEBE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(GPTConfig(vocab_size=50304))  ### NEW  from vocab_size=50257\n",
        "model.to(device)\n",
        "model = torch.compile(model)     ### NEW\n",
        "# logits, loss = model(x, y)\n",
        "# print(loss)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)   ### NEW BETAS and EPS\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate = 6e-4, device = device)\n",
        "for step in range(max_steps):   ### NEW\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)\n",
        "  # import code; code.interact(local=locals())\n",
        "  loss.backward()\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    ### NEW\n",
        "  # determine and set the learning rate for this iterarion\n",
        "  lr = get_lr(step)\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  # torch.cuda.synchronize()     # gpu stops\n",
        "  t1 = time.time()\n",
        "  dt = (t1 - t0) * 1000    # time difference in milliseconds\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "  print(f'step {step:4d} | loss: {loss.item():.6f} | lr {lr:.8f} | norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrTuDJNgHgHn",
        "outputId": "2233fb32-2507-4c67-9cde-b2a867464803",
        "collapsed": true
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 50, with 124,354,560 params\n",
            "num non-decayed parameter tensors: 98, with 121,344 params\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0920 07:27:50.997000 835 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step    0 | loss: 10.964844 | lr 0.00003000 | norm: 35.1516 | dt: 25245.20ms | tok/sec: 5.07\n",
            "step    1 | loss: 10.047119 | lr 0.00006000 | norm: 22.3055 | dt: 43.06ms | tok/sec: 2972.61\n",
            "step    2 | loss: 8.657715 | lr 0.00009000 | norm: 15.1321 | dt: 53.00ms | tok/sec: 2415.29\n",
            "step    3 | loss: 9.625763 | lr 0.00012000 | norm: 26.5920 | dt: 47.51ms | tok/sec: 2694.17\n",
            "step    4 | loss: 9.312622 | lr 0.00015000 | norm: 8.4280 | dt: 61.68ms | tok/sec: 2075.15\n",
            "step    5 | loss: 8.981445 | lr 0.00018000 | norm: 4.9800 | dt: 50.33ms | tok/sec: 2543.45\n",
            "step    6 | loss: 8.897705 | lr 0.00021000 | norm: 3.7241 | dt: 42.39ms | tok/sec: 3019.84\n",
            "step    7 | loss: 8.881714 | lr 0.00024000 | norm: 4.0055 | dt: 52.57ms | tok/sec: 2434.69\n",
            "step    8 | loss: 8.471924 | lr 0.00027000 | norm: 3.6931 | dt: 49.75ms | tok/sec: 2572.79\n",
            "step    9 | loss: 8.695068 | lr 0.00030000 | norm: 3.1325 | dt: 43.04ms | tok/sec: 2973.79\n",
            "step   10 | loss: 8.115356 | lr 0.00030000 | norm: 3.4176 | dt: 43.63ms | tok/sec: 2933.87\n",
            "step   11 | loss: 8.326904 | lr 0.00029958 | norm: 3.0688 | dt: 48.21ms | tok/sec: 2654.89\n",
            "step   12 | loss: 8.003906 | lr 0.00029834 | norm: 3.1492 | dt: 40.46ms | tok/sec: 3163.55\n",
            "step   13 | loss: 7.606750 | lr 0.00029627 | norm: 2.8568 | dt: 40.75ms | tok/sec: 3140.90\n",
            "step   14 | loss: 7.863281 | lr 0.00029339 | norm: 2.6577 | dt: 46.67ms | tok/sec: 2742.37\n",
            "step   15 | loss: 8.356323 | lr 0.00028972 | norm: 3.1268 | dt: 42.23ms | tok/sec: 3030.67\n",
            "step   16 | loss: 7.563660 | lr 0.00028529 | norm: 2.5109 | dt: 55.77ms | tok/sec: 2295.32\n",
            "step   17 | loss: 6.952087 | lr 0.00028011 | norm: 2.6696 | dt: 54.09ms | tok/sec: 2366.29\n",
            "step   18 | loss: 7.286377 | lr 0.00027422 | norm: 2.4339 | dt: 48.36ms | tok/sec: 2646.60\n",
            "step   19 | loss: 7.335266 | lr 0.00026765 | norm: 2.7322 | dt: 46.98ms | tok/sec: 2724.47\n",
            "step   20 | loss: 7.468842 | lr 0.00026046 | norm: 2.7407 | dt: 52.64ms | tok/sec: 2431.73\n",
            "step   21 | loss: 7.439056 | lr 0.00025268 | norm: 3.1820 | dt: 51.40ms | tok/sec: 2490.14\n",
            "step   22 | loss: 6.921112 | lr 0.00024435 | norm: 3.6721 | dt: 53.33ms | tok/sec: 2400.23\n",
            "step   23 | loss: 6.768555 | lr 0.00023554 | norm: 2.4989 | dt: 56.32ms | tok/sec: 2272.58\n",
            "step   24 | loss: 7.835541 | lr 0.00022629 | norm: 2.7152 | dt: 44.84ms | tok/sec: 2854.88\n",
            "step   25 | loss: 7.153595 | lr 0.00021666 | norm: 4.5005 | dt: 32.38ms | tok/sec: 3953.60\n",
            "step   26 | loss: 7.054749 | lr 0.00020672 | norm: 2.7361 | dt: 36.27ms | tok/sec: 3529.17\n",
            "step   27 | loss: 6.313477 | lr 0.00019652 | norm: 2.7868 | dt: 31.41ms | tok/sec: 4074.64\n",
            "step   28 | loss: 6.666382 | lr 0.00018612 | norm: 2.9714 | dt: 31.72ms | tok/sec: 4035.23\n",
            "step   29 | loss: 6.135986 | lr 0.00017559 | norm: 3.7158 | dt: 36.48ms | tok/sec: 3508.32\n",
            "step   30 | loss: 6.689026 | lr 0.00016500 | norm: 2.6297 | dt: 37.74ms | tok/sec: 3391.52\n",
            "step   31 | loss: 7.431885 | lr 0.00015441 | norm: 2.8792 | dt: 47.68ms | tok/sec: 2684.48\n",
            "step   32 | loss: 6.851501 | lr 0.00014388 | norm: 2.5274 | dt: 36.46ms | tok/sec: 3510.27\n",
            "step   33 | loss: 6.101288 | lr 0.00013348 | norm: 2.8405 | dt: 36.66ms | tok/sec: 3491.07\n",
            "step   34 | loss: 6.386841 | lr 0.00012328 | norm: 2.9127 | dt: 32.97ms | tok/sec: 3882.01\n",
            "step   35 | loss: 6.393982 | lr 0.00011334 | norm: 3.1077 | dt: 39.28ms | tok/sec: 3258.40\n",
            "step   36 | loss: 6.633850 | lr 0.00010371 | norm: 2.6952 | dt: 31.47ms | tok/sec: 4067.30\n",
            "step   37 | loss: 7.152740 | lr 0.00009446 | norm: 2.8902 | dt: 31.26ms | tok/sec: 4094.97\n",
            "step   38 | loss: 7.626236 | lr 0.00008565 | norm: 2.9401 | dt: 34.34ms | tok/sec: 3727.78\n",
            "step   39 | loss: 7.883423 | lr 0.00007732 | norm: 3.1601 | dt: 31.76ms | tok/sec: 4030.71\n",
            "step   40 | loss: 7.696777 | lr 0.00006954 | norm: 2.8413 | dt: 31.02ms | tok/sec: 4126.86\n",
            "step   41 | loss: 7.188354 | lr 0.00006235 | norm: 2.8295 | dt: 34.94ms | tok/sec: 3663.12\n",
            "step   42 | loss: 7.820740 | lr 0.00005578 | norm: 2.8514 | dt: 34.80ms | tok/sec: 3677.88\n",
            "step   43 | loss: 7.079773 | lr 0.00004989 | norm: 2.6166 | dt: 39.50ms | tok/sec: 3240.23\n",
            "step   44 | loss: 6.831009 | lr 0.00004471 | norm: 3.0178 | dt: 36.10ms | tok/sec: 3545.25\n",
            "step   45 | loss: 7.129944 | lr 0.00004028 | norm: 2.7627 | dt: 31.41ms | tok/sec: 4074.95\n",
            "step   46 | loss: 6.958191 | lr 0.00003661 | norm: 3.0327 | dt: 30.42ms | tok/sec: 4207.72\n",
            "step   47 | loss: 6.939972 | lr 0.00003373 | norm: 2.8001 | dt: 37.79ms | tok/sec: 3387.28\n",
            "step   48 | loss: 7.004669 | lr 0.00003166 | norm: 2.9095 | dt: 33.23ms | tok/sec: 3852.10\n",
            "step   49 | loss: 7.197021 | lr 0.00003042 | norm: 2.9009 | dt: 31.09ms | tok/sec: 4117.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gw64HGtIV4N",
        "outputId": "ce7e25bc-3cc6-49a8-e2b0-c2411644f722"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.bfloat16"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.transformer.wte.weight.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4HVvllZIaa2",
        "outputId": "93061e13-34d0-47e5-c1b1-c6befe93249d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Worst case scenario\n",
        "-math.log(1/50257)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J9eS4S0mlxG",
        "outputId": "55b6f695-a0fe-4830-a382-6d17d25037a3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.82490511970208"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_return_sequences = 5\n",
        "max_length = 30"
      ],
      "metadata": {
        "id": "-zU7xqZiEq_K"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = GPT.from_pretrained('gpt2')\n",
        "# model.eval()\n",
        "# model.to(device)"
      ],
      "metadata": {
        "id": "3j3B93yHQw-p"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(\"Hello, I'm a data scientist,\")\n",
        "tokens = torch.tensor(tokens, dtype = torch.long) # (8, )\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5, 8)\n",
        "x = tokens.to('cuda')"
      ],
      "metadata": {
        "id": "XyCoRTmmUtOw"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# generate! right now x is (B, T) where B = 5, T = 8\n",
        "# set the seed to 42\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "  # forward the model to get the logits\n",
        "  with torch.no_grad():\n",
        "    logits, _ = model((x))  # (B, T, vocab_size)\n",
        "    # take the logits at the last position\n",
        "    logits = logits[:, -1, :]\n",
        "    # get the porbabilities\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    # do top-k sampling of 50 (huggingface pielien default)\n",
        "    # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "    topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "    # select a token from the top-k probabilities\n",
        "    ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "    # gather the corresponding indices\n",
        "    xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "    # append to the sequence\n",
        "    x = torch.cat((x, xcol), dim=1)"
      ],
      "metadata": {
        "id": "U_635Qo_WGZN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_return_sequences):\n",
        "  tokens = x[i, :max_length].tolist()\n",
        "  decoded = enc.decode(tokens)\n",
        "  print(\">\", decoded)"
      ],
      "metadata": {
        "id": "fhmbTNYTYTcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72515cf-daeb-4f7b-f869-a9432f52846d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Hello, I'm a data scientist,\n",
            "\n",
            " my I\n",
            "I ofI have? myThe the; all of, the as a.\n",
            "\n",
            "> Hello, I'm a data scientist, have not it\n",
            " you not the good\n",
            ", it not him you we\n",
            "..\n",
            " you;\n",
            "\n",
            "> Hello, I'm a data scientist,.\n",
            ":\n",
            "\n",
            " tocius', and', ofUS.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "> Hello, I'm a data scientist,\n",
            " is'd\n",
            "\n",
            " a:\n",
            " my- a\n",
            "cius thatUS the me\n",
            "US do toIN\n",
            "> Hello, I'm a data scientist, the to, and\n",
            " not my?:\n",
            " with, be,'US\n",
            " you;\n",
            "..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pUryNBkPZ0RO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2"
      ],
      "metadata": {
        "id": "T4sQXiszZIK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(data)\n",
        "print(tokens[:24])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGUsGijhizYs",
        "outputId": "ba5d4fbb-ac8c-4ab2-a116-1c8bcc887295"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "buf = torch.tensor(tokens[:24 + 1])\n",
        "x = buf[:-1].view(4, 6)\n",
        "y = buf[1:].view(4, 6)\n",
        "print(x)\n",
        "print(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBaFzAwpjNop",
        "outputId": "696f300e-1c7b-4410-c6ab-9fc6c2f3ee8f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
            "        [ 5120,   597,  2252,    11,  3285,   502],\n",
            "        [ 2740,    13,   198,   198,  3237,    25],\n",
            "        [  198,  5248,   461,    11,  2740,    13]])\n",
            "tensor([[22307,    25,   198,  8421,   356,  5120],\n",
            "        [  597,  2252,    11,  3285,   502,  2740],\n",
            "        [   13,   198,   198,  3237,    25,   198],\n",
            "        [ 5248,   461,    11,  2740,    13,   198]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader"
      ],
      "metadata": {
        "id": "J52xQFxSjjCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPKBXN7zCHdT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}